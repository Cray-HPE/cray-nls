apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ncn-lifecycle-rebuild-
  labels:
    targetNcn: ncn-w001
spec:
  podGC:
    # Pod GC strategy must be one of the following:
    # * OnPodCompletion - delete pods immediately when pod is completed (including errors/failures)
    # * OnPodSuccess - delete pods immediately when pod is successful
    # * OnWorkflowCompletion - delete pods when workflow is completed
    # * OnWorkflowSuccess - delete pods when workflow is successful
    strategy: OnPodCompletion
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: NotIn
            values:
            - ncn-w001
  entrypoint: rebuild
  volumes:
    - name: ssh
      hostPath:
        path: /root/.ssh
        type: Directory
    - name: k8s
      hostPath:
        path: /etc/kubernetes
        type: Directory
    - name: user-bin
      hostPath:
        path: /usr/bin
        type: Directory
  templates:
    - name: rebuild
      dag:
        tasks:
          - name: wait-for-cfs
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    /usr/share/doc/csm/upgrade/1.2/scripts/cfs/wait_for_configuration.sh --xnames $(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ncn-w001 'cat /etc/cray/xname')
          - name: ensure-nexus-can-start-on-any-node
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    workers="$(kubectl get node --selector='!node-role.kubernetes.io/master' -o name | sed -e 's,^node/,,' | paste -sd,)"
                    export PDSH_SSH_ARGS_APPEND="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
                    kubectl get configmap -n nexus cray-precache-images -o json | jq -r '.data.images_to_cache' | while read image; do echo >&2 "+ caching $image"; pdsh -w "$workers" "crictl pull $image" 2>/dev/null; done
          - name: ensure-etcd-pods
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    while [[ "$(kubectl get po -A -l 'app=etcd' | grep -v "Running"| wc -l)" != "1" ]]; do
                        echo "Some etcd pods are not in running state, wait for 5s ..."
                        kubectl get po -A -l 'app=etcd' | grep -v "Running"
                        sleep 5
                    done

                    etcdClusters=$(kubectl get Etcdclusters -n services | grep "cray-"|awk '{print $1}')
                    for cluster in $etcdClusters
                    do
                        numOfPods=$(kubectl get pods -A -l 'app=etcd'| grep $cluster | grep "Running" | wc -l)
                        if [[ $numOfPods -ne 3 ]];then
                            echo "ERROR - Etcd cluster: $cluster should have 3 pods running but only $numOfPods are running"
                            exit 1
                        fi
                    done
          - name: ensure-pg-pods
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    wget -q http://rgw-vip.nmn/ncn-utils/csi;chmod 0755 csi
                    mv csi /usr/local/bin/csi
                    csi pit validate --postgres
          - name: drain
            dependencies:
              - wait-for-cfs
              - ensure-nexus-can-start-on-any-node
              - ensure-etcd-pods
              - ensure-pg-pods
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \
                    -d client_id=admin-client \
                    -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath='{.data.client-secret}' | base64 -d` \
                    https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r '.access_token')

                    csi automate ncn kubernetes --action delete-ncn --ncn ncn-w001 --kubeconfig /etc/kubernetes/admin.conf
          - name: update-bss
            dependencies:
              - wait-for-cfs
              - ensure-nexus-can-start-on-any-node
              - ensure-etcd-pods
              - ensure-pg-pods
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \
                    -d client_id=admin-client \
                    -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath='{.data.client-secret}' | base64 -d` \
                    https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r '.access_token')

                    csi handoff bss-update-param --set metal.no-wipe=0 --limit $(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ncn-w001 'cat /etc/cray/xname')
          - name: wipe-and-rebuild
            dependencies:
              - update-bss
              - drain
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    set -e
                    mv /etc/cray/upgrade/csm/ncn-w001/state /etc/cray/upgrade/csm/ncn-w001/state.bak || true
                    NONINTERACTIVE=1 /usr/share/doc/csm/upgrade/1.2/scripts/common/ncn-rebuild-common.sh ncn-w001 --rebuild
          - name: update-bss-no-wipe
            retryStrategy:
              limit: "2"
              retryPolicy: "Always"
            dependencies:
              - wipe-and-rebuild
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    export TOKEN=$(curl -k -s -S -d grant_type=client_credentials \
                    -d client_id=admin-client \
                    -d client_secret=`kubectl get secrets admin-client-auth -o jsonpath='{.data.client-secret}' | base64 -d` \
                    https://api-gw-service-nmn.local/keycloak/realms/shasta/protocol/openid-connect/token | jq -r '.access_token')

                    csi handoff bss-update-param --set metal.no-wipe=1 --limit $(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ncn-w001 'cat /etc/cray/xname')
          - name: wait-for-cfs-2-and-more
            dependencies:
              - wipe-and-rebuild
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    /usr/share/doc/csm/upgrade/1.2/scripts/cfs/wait_for_configuration.sh --xnames $(ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ncn-w001 'cat /etc/cray/xname')
          - name: wait-for-key-pods
            dependencies:
              - wipe-and-rebuild
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    while true; do
                      output=$(kubectl get po -A -o wide | grep -e etcd -e speaker | grep ncn-w001 | awk '{print $4}')
                      if [ ! -n "$output" ]; then
                        #
                        # No pods scheduled to start on this node, we are done
                        #
                        break
                      fi
                      set +e
                      echo "$output" | grep -v -e Running -e Completed > /dev/null
                      rc=$?
                      set -e
                      if [[ "$rc" -eq 1 ]]; then
                        echo "All etcd and speaker pods are running on ncn-w001"
                        break
                      fi
                      echo "Some etcd and speaker pods are not running on ncn-w001 -- sleeping for 10 seconds..."
                      sleep 10
                    done
                    scp /root/docs-csm-latest.noarch.rpm ncn-w001:/root/docs-csm-latest.noarch.rpm
                    ssh ncn-w001 "rpm --force -Uvh /root/docs-csm-latest.noarch.rpm"
          - name: goss
            dependencies:
              - wait-for-cfs-2-and-more
              - wait-for-key-pods
            templateRef:
              name: ssh-with-kubeconfig
              template: bash-script-example
            arguments:
              parameters:
                - name: message
                  value: |
                    ssh ncn-w001 -t "SW_ADMIN_PASSWORD='!nitial0' GOSS_BASE=/opt/cray/tests/install/ncn goss -g /opt/cray/tests/install/ncn/suites/ncn-upgrade-tests-worker.yaml --vars=/opt/cray/tests/install/ncn/vars/variables-ncn.yaml validate"
